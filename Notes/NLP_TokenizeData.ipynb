{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled2.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPlpXbx9+u84Aj5IYy0A/6A"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iHEoCUDp5mns","executionInfo":{"status":"ok","timestamp":1633981754332,"user_tz":300,"elapsed":26161,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}},"outputId":"2a356527-7d6e-4948-e26f-351ccf071c6f"},"source":["import os\n","# Find the latest version of spark 3.0  from http://www.apache.org/dist/spark/ and enter as the spark version\n","# For example:\n","# spark_version = 'spark-3.0.3'\n","spark_version = 'spark-3.0.3'\n","os.environ['SPARK_VERSION']=spark_version\n","\n","# Install Spark and Java\n","!apt-get update\n","!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n","!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n","!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n","!pip install -q findspark\n","\n","# Set Environment Variables\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n","\n","# Start a SparkSession\n","import findspark\n","findspark.init()"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n","Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Get:9 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:10 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Get:11 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,431 kB]\n","Hit:12 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,210 kB]\n","Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,801 kB]\n","Fetched 6,698 kB in 4s (1,696 kB/s)\n","Reading package lists... Done\n"]}]},{"cell_type":"code","metadata":{"id":"j4SIjSrj5pmK","executionInfo":{"status":"ok","timestamp":1633981768110,"user_tz":300,"elapsed":8658,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}}},"source":["# Start Spark session\n","from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName(\"Tokens\").getOrCreate()"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"fBmQ9noN5qa0","executionInfo":{"status":"ok","timestamp":1633981774056,"user_tz":300,"elapsed":272,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}}},"source":["# import tokenizer library\n","from pyspark.ml.feature import Tokenizer\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LPo3hVV852zG","executionInfo":{"status":"ok","timestamp":1633981887479,"user_tz":300,"elapsed":5906,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}},"outputId":"afbf8fbc-7278-4421-816c-8cf10d12bad6"},"source":["# Create sample DataFrame\n","dataframe = spark.createDataFrame([\n","    (0, \"Spark is great\"),\n","    (1, \"We are learning Spark\"),\n","    (2, \"Spark is better than hadoop no doubt\")\n","], [\"id\", \"sentence\"])\n","\n","dataframe.show()"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+--------------------+\n","| id|            sentence|\n","+---+--------------------+\n","|  0|      Spark is great|\n","|  1|We are learning S...|\n","|  2|Spark is better t...|\n","+---+--------------------+\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uPQ1nxnc6T8f","executionInfo":{"status":"ok","timestamp":1633981938165,"user_tz":300,"elapsed":107,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}},"outputId":"13346677-f4e3-472e-b14e-129f86767c30"},"source":["# Tokenize sentences - example with transformation method only\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","tokenizer"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Tokenizer_96a024524d5f"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hifP5yc66f6C","executionInfo":{"status":"ok","timestamp":1633982107486,"user_tz":300,"elapsed":601,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}},"outputId":"f413d3db-6001-4844-8da8-c36830f501b1"},"source":["# Tokenize sentences - using show(truncated=False) as our action to display the results without shortening the output\n","tokenizer_df = tokenizer.transform(dataframe)\n","tokenizer_df.show(truncate=False)"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------+--------------------------------------------+\n","|id |sentence                            |words                                       |\n","+---+------------------------------------+--------------------------------------------+\n","|0  |Spark is great                      |[spark, is, great]                          |\n","|1  |We are learning Spark               |[we, are, learning, spark]                  |\n","|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|\n","+---+------------------------------------+--------------------------------------------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"Pui7U7Un7a3Y","executionInfo":{"status":"ok","timestamp":1633982353064,"user_tz":300,"elapsed":116,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}}},"source":["# Create a function to return the length of a list\n","def word_list_length(word_list):\n","    return len(word_list)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"rLWG6QVF7a-p","executionInfo":{"status":"ok","timestamp":1633982355427,"user_tz":300,"elapsed":118,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}}},"source":["# import the udf function, the col function to select a column to be passed into a function,\n","# and the type IntegerType that will be used in our udf to define the data type of the output\n","from pyspark.sql.functions import col, udf\n","from pyspark.sql.types import IntegerType"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"CfTZRCxE7bCi","executionInfo":{"status":"ok","timestamp":1633982357265,"user_tz":300,"elapsed":117,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}}},"source":["# Using the udf function, we can create our function to be passed in.\n","# The udf will take in the name of the function as a parameter and the output data type,\n","# which is the IntegerType that we just imported.\n","\n","# Create a user defined function\n","count_tokens = udf(word_list_length, IntegerType())"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kOqOuLo98DsA","executionInfo":{"status":"ok","timestamp":1633982476859,"user_tz":300,"elapsed":893,"user":{"displayName":"Keyton Porras","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjfD1QH-gIhp2-YXUx8WZgvyo4MWcU4pfNh-Iez=s64","userId":"16584665332485333005"}},"outputId":"7471de49-c073-4217-cbdd-74fcfbd97703"},"source":["# Create our Tokenizer\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","\n","# Transform DataFrame\n","tokenized_df = tokenizer.transform(dataframe)\n","\n","# Select the needed columns and don't truncate results\n","tokenized_df.withColumn(\"tokens\", count_tokens(col(\"words\"))).show(truncate=False)"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+------------------------------------+--------------------------------------------+------+\n","|id |sentence                            |words                                       |tokens|\n","+---+------------------------------------+--------------------------------------------+------+\n","|0  |Spark is great                      |[spark, is, great]                          |3     |\n","|1  |We are learning Spark               |[we, are, learning, spark]                  |4     |\n","|2  |Spark is better than hadoop no doubt|[spark, is, better, than, hadoop, no, doubt]|7     |\n","+---+------------------------------------+--------------------------------------------+------+\n","\n"]}]},{"cell_type":"code","metadata":{"id":"1wsgs31K9mjU"},"source":[""],"execution_count":null,"outputs":[]}]}